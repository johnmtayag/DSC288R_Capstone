{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc2a634-e3fa-4e5e-a9c1-46a98cd78613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from scripts.preprocessing import xdog, filter_out_diaphragm, scale_range, histogram_equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a9646d-b52d-4229-b22f-29c02dbe03ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EDIT DIRECTORY VARIABLES\n",
    "root = \"S:/CheXpert\"\n",
    "source_train_folder_name = \"CheXpert-v1.0 batch 4 (train 3)\"\n",
    "source_test_folder_name = \"CheXpert-v1.0 batch 1 (validate & csv)\"\n",
    "\n",
    "train_filepath = f\"{root}/train_data.csv\"\n",
    "test_filepath = f\"{root}/test_data.csv\"\n",
    "\n",
    "train_folder_name = \"train\"\n",
    "test_folder_name = \"test\"\n",
    "train2_folder_name = \"train2\"\n",
    "test2_folder_name = \"test2\"\n",
    "\n",
    "#####\n",
    "\n",
    "### Instantiate Variables\n",
    "\n",
    "# Source: This is where the raw image files are stored (the next level are patient folders)\n",
    "source_train_root = f\"{root}/{source_train_folder_name}/\"\n",
    "source_test_root  = f\"{root}/{source_test_folder_name}/valid/\"\n",
    "\n",
    "# These are the output roots for file paths being added to the train/test files\n",
    "train_root = f\"{root}/{train_folder_name}/\"\n",
    "test_root = f\"{root}/{test_folder_name}/\"\n",
    "train2_root = f\"{root}/{train2_folder_name}/\"\n",
    "test2_root = f\"{root}/{test2_folder_name}/\"\n",
    "\n",
    "# This is the original root for the train/test csv files\n",
    "base_path = \"CheXpert-v1.0/train/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5da7fa9e-6b1d-46fb-9f6f-a4d8ad97957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in train_df: 39371\n",
      "# rows in test_df: 202\n"
     ]
    }
   ],
   "source": [
    "### Load the training/validation csvs\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "test_df = pd.read_csv(test_filepath)\n",
    "\n",
    "print(f\"# rows in train_df: {len(train_df)}\")\n",
    "print(f\"# rows in test_df: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e1684-c297-49e6-bf62-af5862f41fa1",
   "metadata": {},
   "source": [
    "## Preprocessing Steps:\n",
    "\n",
    "These are performed for both the regular sets and the preprocessed sets\n",
    "\n",
    "* Scale the image values to the range [0-255]\n",
    "* Crop out any borders algorithmically\n",
    "* Resize the training and validation images to 512x512\n",
    "* Find the 90-degree rotation which is closest to the average of a sample of 1000 x-ray images\n",
    "\n",
    "These additional actions are performed for the preprocessed sets ONLY\n",
    "\n",
    "* Adaptive mask to remove the diaphragm\n",
    "* Increasing contrast via histogram equalization\n",
    "* Edge detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45084a08-28de-4823-922b-889b68b2f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing variables\n",
    "\n",
    "### Value range for scaling image array\n",
    "scale_min = 0\n",
    "scale_max = 255\n",
    "\n",
    "### Desired image dimensions for analysis\n",
    "image_height = 512\n",
    "image_width = 512\n",
    "\n",
    "# difference_of_gaussians variables\n",
    "dog_low_sigma = 0.5\n",
    "dog_high_sigma = 100 * dog_low_sigma #50\n",
    "dog_truncate = 1\n",
    "\n",
    "# custom gaussian variables\n",
    "xdog_k = 50    ### k scales Gaussians - this increases contrast\n",
    "xdog_sigma = 0.5     ### sigma determines the initial blur strength - higher values discard finer details\n",
    "xdog_tau = 10     ### tau scales the values of the second blurred image - higher values increase sharpness\n",
    "xdog_ep = 0.75 * (scale_max - scale_min)     ### ep is the cutoff that determines what values become hard edges\n",
    "xdog_phi = 0.02      ### phi is a parameter in tanh that affects how much the darker areas can be seen - smaller values retain more detail\n",
    "xdog_threshold = \"tanh\"     ### threshold type: None, binary, tanh\n",
    "xdog_truncate = 1      ### truncate: This determines the kernel of the Gaussian filter which affects smoothing\n",
    "\n",
    "# histogram equalization\n",
    "he_sigma = 5\n",
    "\n",
    "# filtering out diaphragm\n",
    "mask_threshold = 0.9\n",
    "\n",
    "# bilateral filtering for denoising\n",
    "db_sigma = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8b94e-e665-4551-a200-dd567b0e0b1d",
   "metadata": {},
   "source": [
    "### Regular sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0efa382-c0e0-4a93-aeee-e7cc2861e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing steps for the regular test set\n",
    "input_paths = test_df[\"test_file_path\"]\n",
    "output_paths = test_df[\"test2_file_path\"]\n",
    "\n",
    "for i,(input_file_path, output_file_path) in enumerate(zip(input_paths, output_paths)):\n",
    "    with Image.open(input_file_path) as img:\n",
    "        img_arr = np.array(img)\n",
    "        img_arr = scale_range(img_arr, scale_min, scale_max)\n",
    "        img_arr = crop_borders(img_arr, threshold_range)\n",
    "        img_arr = np.array(Image.fromarray(img_arr).resize((image_height, image_width)))\n",
    "        img_arr, dist = get_best_rotation(img_arr, avg_img_arr)\n",
    "        img = Image.fromarray(img_arr.astype(np.uint8))\n",
    "        img.save(output_file_path, \"JPEG\", quality=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc96f417-cfd6-4244-b550-d025c5e4e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing steps for the regular training set (RUN THIS ONCE, MAY TAKE A FEW HOURS)\n",
    "input_paths = train_df[\"train_file_path\"]\n",
    "output_paths = train_df[\"train2_file_path\"]\n",
    "\n",
    "for i,(input_file_path, output_file_path) in enumerate(zip(input_paths, output_paths)):\n",
    "    with Image.open(input_file_path) as img:\n",
    "        img_arr = np.array(img)\n",
    "        img_arr = scale_range(img_arr, scale_min, scale_max)\n",
    "        img_arr = crop_borders(img_arr, threshold_range)\n",
    "        img_arr = np.array(Image.fromarray(img_arr).resize((image_height, image_width)))\n",
    "        img_arr, dist = get_best_rotation(img_arr, avg_img_arr)\n",
    "        img = Image.fromarray(img_arr.astype(np.uint8))\n",
    "        img.save(output_file_path, \"JPEG\", quality=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
